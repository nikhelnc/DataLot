# CAHIER DES CHARGES ‚Äî Mod√®les de Pr√©diction Statistique

## Lotto Analyzer ‚Äî Extension des mod√®les probabilistes

**Application Python pour l'analyse et la simulation de loteries**

> Version 2.0 ‚Äî F√©vrier 2026
> Auteur : Nicolas ‚Äî Cal√©donienne des Eaux / Doctorat
> *Confidentiel ‚Äî Usage acad√©mique*

---

## Table des mati√®res

1. [Introduction et contexte](#1-introduction-et-contexte)
2. [Interface commune des mod√®les (ModelBase)](#2-interface-commune-des-mod√®les-modelbase)
3. [Mod√®les existants (rappel synth√©tique)](#3-mod√®les-existants-rappel-synth√©tique)
4. [Nouveaux mod√®les √† impl√©menter](#4-nouveaux-mod√®les-√†-impl√©menter)
   - [M3 ‚Äî Exponential Decay](#m3--exponential-decay)
   - [M4 ‚Äî Hidden Markov Model (HMM)](#m4--hidden-markov-model-hmm)
   - [M7 ‚Äî Entropy-Based Selection](#m7--entropy-based-selection)
   - [M8 ‚Äî Changepoint Detection](#m8--changepoint-detection)
   - [M9 ‚Äî Bayesian Network](#m9--bayesian-network)
   - [M12 ‚Äî Mixture of Dirichlet](#m12--mixture-of-dirichlet)
   - [M13 ‚Äî Spectral / Fourier Analysis](#m13--spectral--fourier-analysis)
   - [M14 ‚Äî Copula Model](#m14--copula-model)
   - [M15 ‚Äî Thompson Sampling](#m15--thompson-sampling)
5. [Tableau r√©capitulatif complet](#5-tableau-r√©capitulatif-complet)

---

## 1. Introduction et contexte

### 1.1 Objectif du document

Ce cahier des charges d√©crit l'ensemble des mod√®les statistiques √† impl√©menter dans l'application Lotto Analyzer v2.0. Il couvre les mod√®les existants (M0 √† M10, ANTI, ANTI2) ainsi que les nouveaux mod√®les propos√©s (M3, M4, M7, M8, M9, M15, M12, M13, M14). Chaque mod√®le est sp√©cifi√© avec ses fondements th√©oriques, ses param√®tres, ses formules, et ses contraintes d'impl√©mentation en Python.

L'objectif final est double : √©valuer rigoureusement ces mod√®les sur des donn√©es historiques de loteries australiennes (Powerball, TattsLotto, Oz Lotto) via un protocole de backtest walk-forward, et produire des distributions de probabilit√© estim√©es pour les prochains tirages.

### 1.2 Loteries cibles

| Loterie | Grille principale | Bonus | Compl√©mentaires |
|---------|------------------|-------|-----------------|
| **Powerball (AU)** | 7 parmi 1-35 | 1 parmi 1-20 | ‚Äî |
| **TattsLotto** | 6 parmi 1-45 | ‚Äî | 2 (7√®me + 8√®me) |
| **Oz Lotto** | 7 parmi 1-47 | ‚Äî | 3 (8/9/10√®me) |

### 1.3 Architecture applicative

L'application est d√©velopp√©e en Python et s'articule autour des modules suivants :

- **core/models/** : un fichier Python par mod√®le, respectant l'interface commune ModelBase.
- **core/backtest.py** : moteur de backtest walk-forward, appelle chaque mod√®le de mani√®re standardis√©e.
- **core/metrics.py** : calcul de Brier score, ECE, Lift, hit rate, divisions de prix.
- **core/montecarlo.py** : simulations Monte Carlo sous H0 pour r√©f√©rencement.
- **api/** : endpoints pour l'interface web (FastAPI ou Flask).
- **data/** : historiques CSV des tirages, metadata des r√®gles par loterie.

---

## 2. Interface commune des mod√®les (ModelBase)

Tous les mod√®les doivent impl√©menter l'interface suivante pour √™tre compatibles avec le moteur de backtest et l'interface utilisateur :

```python
class ModelBase(ABC):
    model_id: str          # ex: 'M3'
    model_name: str         # ex: 'Exponential Decay'
    model_type: str         # ex: 'Pond√©ration temporelle'

    @abstractmethod
    def fit(self, draws: List[Draw], game_rules: GameRules) -> None:
        """Entra√Æne le mod√®le sur l'historique."""

    @abstractmethod
    def predict_proba(self) -> Dict[str, np.ndarray]:
        """Retourne les probabilit√©s par num√©ro.
        Returns: {'main': array[N], 'bonus': array[B]}"""

    @abstractmethod
    def generate_combinations(self, n: int = 1) -> List[Combination]:
        """G√©n√®re n combinaisons pond√©r√©es."""

    def get_params(self) -> Dict[str, Any]:
        """Retourne les param√®tres pour reproductibilit√©."""
```

Chaque mod√®le produit obligatoirement un vecteur de probabilit√©s normalis√© (somme = 1) pour les num√©ros principaux et, le cas √©ch√©ant, pour les num√©ros bonus.

### 2.1 Structure de donn√©es Draw

```python
@dataclass
class Draw:
    date: datetime
    main_numbers: List[int]   # num√©ros principaux tri√©s
    bonus_numbers: List[int]   # bonus / compl√©mentaires
    draw_number: int            # identifiant s√©quentiel
```

### 2.2 Structure GameRules

```python
@dataclass
class GameRules:
    name: str                   # 'powerball_au', 'tattslotto', 'ozlotto'
    main_range: Tuple[int, int]  # (1, 35) pour Powerball
    main_pick: int               # 7 pour Powerball
    bonus_range: Tuple[int, int]  # (1, 20) pour Powerball
    bonus_pick: int               # 1 pour Powerball
    supplementary_count: int      # 0, 2 ou 3
```

---

## 3. Mod√®les existants (rappel synth√©tique)

Les mod√®les suivants sont d√©j√† impl√©ment√©s et servent de r√©f√©rence. Ils sont d√©crits ici de mani√®re synth√©tique pour compl√©tude.

| ID | Nom | Type | Principe |
|----|-----|------|----------|
| **M0** | Baseline | Al√©atoire | S√©lection uniforme. R√©f√©rence obligatoire pour tout benchmark. |
| **M1** | Dirichlet | Bay√©sien | Prior Dirichlet + fr√©quences observ√©es. Lissage naturel. |
| **M2** | Windowed | Fen√™tre glissante | Pond√©ration r√©cente avec shrinkage vers la moyenne globale. |
| **M5** | Co-occurrence | Paires | D√©tection de paires sur-repr√©sent√©es (delta obs-attendu). |
| **M6** | Gaps & Streaks | √âcarts | Num√©ros ¬´ en retard ¬ª vs √©cart moyen historique. |
| **M10** | Ensemble | M√©ta-mod√®le | Stacking de M0 + M1 + M2 avec poids optimis√©s. |
| **ANTI** | Anti-Consensus | Contrarian | Num√©ros non pr√©dits par les autres mod√®les. |
| **ANTI2** | Anti-Consensus v2 | Contrarian+ | ANTI avec contrainte de diversit√© inter-combinaisons. |

---

## 4. Nouveaux mod√®les √† impl√©menter

Cette section constitue le c≈ìur du cahier des charges. Chaque mod√®le est sp√©cifi√© avec son identifiant, son fondement th√©orique, ses param√®tres configurables, ses formules, et ses recommandations d'impl√©mentation.

---

### M3 ‚Äî Exponential Decay

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M3 |
| **Nom complet** | Exponential Decay |
| **Type** | Pond√©ration temporelle exponentielle |
| **Librairies Python** | numpy |
| **Complexit√©** | O(N¬∑K) o√π N=tirages, K=num√©ros |

#### Principe

Contrairement √† M2 qui utilise une fen√™tre fixe (les N derniers tirages comptent √©galement, les pr√©c√©dents sont ignor√©s), M3 attribue un poids √† chaque tirage pass√© selon une d√©croissance exponentielle. Cela √©limine l'effet de bord brutal de la fen√™tre et cr√©e une transition douce entre tirages r√©cents et anciens.

Le param√®tre lambda contr√¥le la vitesse de d√©croissance : un lambda √©lev√© donne plus de poids aux tirages tr√®s r√©cents, un lambda faible se rapproche du mod√®le uniforme.

Ce mod√®le est particuli√®rement pertinent pour d√©tecter des d√©rives lentes dans les fr√©quences, comme celles qui pourraient r√©sulter d'un changement d'√©quipement progressif.

#### Param√®tres

- **lambda_decay** (float, d√©faut: 0.02) : Taux de d√©croissance exponentielle. Optimisable par validation crois√©e walk-forward.
- **min_weight** (float, d√©faut: 1e-6) : Seuil minimum de poids. Les tirages en dessous sont ignor√©s pour performance.

#### Formule / Pseudo-code

```
w(t) = exp(-Œª * (T - t))
P(i) = Œ£_t [ w(t) * ùüô{i ‚àà tirage_t} ] / Œ£_t w(t)
Normalisation : P = P / sum(P)
```

#### Avantages

- √âlimine l'artefact de bord de la fen√™tre fixe (M2).
- Param√®tre unique et interpr√©table.
- Transition douce entre r√©gime ¬´ m√©moire courte ¬ª et ¬´ m√©moire longue ¬ª.
- Compatible nativement avec le walk-forward (pas de fen√™tre √† d√©finir).

#### Notes d'impl√©mentation

Impl√©menter la recherche du lambda optimal par grid search ou Brent optimization sur le Brier score du backtest. Pour √©viter les probl√®mes num√©riques, travailler en log-espace pour les poids.

---

### M4 ‚Äî Hidden Markov Model (HMM)

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M4 |
| **Nom complet** | Hidden Markov Model |
| **Type** | Mod√®le √† √©tats cach√©s |
| **Librairies Python** | hmmlearn, numpy, scikit-learn |
| **Complexit√©** | O(N¬∑S¬≤) par it√©ration EM, S=√©tats |

#### Principe

M4 mod√©lise l'hypoth√®se que les tirages peuvent provenir de plusieurs ¬´ r√©gimes ¬ª latents, chacun avec sa propre distribution de probabilit√© sur les num√©ros. Les transitions entre r√©gimes suivent une cha√Æne de Markov.

Cas d'usage : si une loterie change de machine de tirage, de proc√©dure, ou de conditions environnementales, le HMM peut capturer ces transitions √† travers ses √©tats cach√©s.

Pour chaque √©tat cach√©, le mod√®le apprend une distribution multinomiale sur les num√©ros. La pr√©diction est la distribution √©mise par l'√©tat le plus probable au temps t+1.

Le nombre d'√©tats est un hyperparam√®tre critique : trop d'√©tats entra√Ænent du surapprentissage, trop peu ne capturent pas les r√©gimes.

#### Param√®tres

- **n_states** (int, d√©faut: 3) : Nombre d'√©tats cach√©s. Tester 2 √† 5, s√©lectionner par BIC.
- **n_iter** (int, d√©faut: 100) : It√©rations maximum pour l'algorithme Baum-Welch (EM).
- **tol** (float, d√©faut: 1e-4) : Crit√®re de convergence de la log-vraisemblance.
- **random_state** (int) : Seed pour reproductibilit√© de l'initialisation.

#### Formule / Pseudo-code

```
# Matrice de transition : A[i,j] = P(state_t+1=j | state_t=i)
# √âmission : B[s] = Multinomial(K num√©ros) pour chaque √©tat s
# Entra√Ænement : Baum-Welch (EM)
# Pr√©diction : Œ±_T ¬∑ A -> distribution sur √©tats t+1
#              P(num) = Œ£_s P(state=s|t+1) * B[s](num)
```

#### Avantages

- Capture des changements de r√©gime non d√©tectables par les mod√®les stationnaires.
- Cadre probabiliste rigoureux avec vraisemblance.
- S√©lection de mod√®le par BIC/AIC.
- Forte valeur acad√©mique pour la th√®se (test de l'hypoth√®se de r√©gimes).

#### Notes d'impl√©mentation

Utiliser hmmlearn (GaussianHMM ou MultinomialHMM). Si MultinomialHMM n'est pas disponible, encoder les tirages comme vecteurs binaires et utiliser GaussianHMM ou impl√©menter un HMM custom.

Attention : la convergence de Baum-Welch est sensible √† l'initialisation. Lancer 10 inits al√©atoires et garder celle avec la meilleure log-vraisemblance.

Pour le walk-forward, r√©entra√Æner le HMM √† chaque pas (co√ªteux) ou mettre √† jour de mani√®re incr√©mentale le forward pass.

---

### M7 ‚Äî Entropy-Based Selection

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M7 |
| **Nom complet** | Entropy-Based Selection |
| **Type** | S√©lection par entropie locale |
| **Librairies Python** | numpy, scipy |
| **Complexit√©** | O(N¬∑K¬∑W) |

#### Principe

M7 calcule l'entropie de Shannon sur des fen√™tres glissantes pour chaque num√©ro individuellement. L'intuition est que si un num√©ro a un comportement localement plus ¬´ r√©gulier ¬ª (entropie basse), il pourrait indiquer une anomalie exploitable.

Pour chaque num√©ro i, on construit la s√©rie binaire x_i(t) = 1 si le num√©ro i a √©t√© tir√© au tirage t, 0 sinon. Sur une fen√™tre glissante, on calcule la fr√©quence p et l'entropie H = -p¬∑log(p) - (1-p)¬∑log(1-p).

Les num√©ros dont l'entropie locale est significativement plus basse que l'entropie th√©orique sous H0 sont favoris√©s.

#### Param√®tres

- **window_size** (int, d√©faut: 30) : Taille de la fen√™tre pour le calcul d'entropie locale.
- **alpha_threshold** (float, d√©faut: 0.05) : Seuil de significativit√© pour l'√©cart √† H0.
- **selection_mode** (str, d√©faut: 'low_entropy') : Strat√©gie : favoriser les num√©ros √† faible entropie ('low_entropy') ou forte entropie ('high_entropy').

#### Formule / Pseudo-code

```
x_i(t) = 1{num√©ro i tir√© au tirage t}
p_i = mean(x_i[t-W:t])
H_i = -p_i * log2(p_i) - (1-p_i) * log2(1-p_i)
H0_theorique = -p0 * log2(p0) - (1-p0) * log2(1-p0)
   o√π p0 = K/N (ex: 7/35 pour Powerball)
score_i = H0_theorique - H_i  (positif = plus r√©gulier qu'attendu)
P(i) ‚àù softmax(score_i / temperature)
```

#### Avantages

- D√©tecte les num√©ros dont le comportement d√©vie de l'al√©atoire pur.
- Interpr√©table : l'entropie est une mesure d'information standard.
- Compl√©mentaire aux mod√®les fr√©quentistes (M1, M2) car capture la r√©gularit√©, pas la fr√©quence.
- Bon signal d'alerte pour la th√®se (d√©tection d'anomalies).

#### Notes d'impl√©mentation

Impl√©menter via scipy.stats.entropy ou calcul direct numpy. Comparer H_i √† la distribution de H sous Monte Carlo (H0) pour obtenir une p-value par num√©ro. Convertir les scores en probabilit√©s via softmax avec un param√®tre de temp√©rature.

---

### M8 ‚Äî Changepoint Detection

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M8 |
| **Nom complet** | Changepoint Detection |
| **Type** | D√©tection de ruptures (CUSUM/PELT) |
| **Librairies Python** | ruptures, numpy |
| **Complexit√©** | O(N¬∑K) pour PELT (quasi-lin√©aire) |

#### Principe

M8 identifie les points de rupture structurelle dans les s√©ries temporelles de fr√©quence de chaque num√©ro. Apr√®s d√©tection d'une rupture, le mod√®le recalcule les probabilit√©s uniquement sur le segment post-rupture.

L'hypoth√®se sous-jacente est que certains changements (machine, proc√©dure, op√©rateur) peuvent modifier la distribution des tirages. Si un point de rupture est d√©tect√©, les donn√©es ant√©rieures ne sont plus pertinentes pour la pr√©diction.

L'algorithme PELT (Pruned Exact Linear Time) est utilis√© pour sa performance sur de longues s√©ries. En compl√©ment, un CUSUM global peut servir de d√©tection en ligne.

Ce mod√®le a une forte valeur pour la th√®se car il teste directement l'hypoth√®se de stationnarit√©.

#### Param√®tres

- **model_type** (str, d√©faut: 'l2') : Mod√®le de co√ªt pour PELT : 'l1', 'l2', 'rbf', 'normal'.
- **min_segment_size** (int, d√©faut: 20) : Taille minimale d'un segment entre deux ruptures.
- **penalty** (str/float, d√©faut: 'bic') : P√©nalit√© pour PELT : 'bic', 'aic', ou valeur num√©rique.
- **cusum_threshold** (float, d√©faut: 4.0) : Seuil pour la d√©tection CUSUM en ligne (en √©carts-types).

#### Formule / Pseudo-code

```
# D√©tection offline (PELT) :
breakpoints = ruptures.Pelt(model=model_type, min_size=min_seg)
                      .fit(freq_series).predict(pen=penalty)

# Segment courant : donn√©es depuis le dernier breakpoint
current_segment = draws[last_breakpoint:]
P(i) = freq_i(current_segment) / sum(freqs)

# D√©tection online (CUSUM) :
S(t) = max(0, S(t-1) + (x(t) - mu0) - k)
Alarme si S(t) > h
```

#### Avantages

- Test direct de l'hypoth√®se de stationnarit√©.
- √âlimine les donn√©es obsol√®tes apr√®s un changement de r√©gime.
- Algorithme PELT en temps quasi-lin√©aire.
- Applicable √† chaque num√©ro ind√©pendamment ou globalement.
- Valeur acad√©mique tr√®s √©lev√©e pour la th√®se.

#### Notes d'impl√©mentation

Utiliser la librairie ruptures pour PELT. Impl√©menter CUSUM manuellement (quelques lignes). Si aucune rupture d√©tect√©e, fallback sur M1 (Dirichlet global). Stocker les breakpoints d√©tect√©s dans les m√©tadonn√©es du mod√®le pour audit. Corr√©ler les ruptures d√©tect√©es avec des informations externes (changements de r√®gles, machines) pour validation.

---

### M9 ‚Äî Bayesian Network

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M9 |
| **Nom complet** | Bayesian Network |
| **Type** | R√©seau bay√©sien de d√©pendances |
| **Librairies Python** | pgmpy, numpy, networkx |
| **Complexit√©** | O(K¬≤¬∑N) pour structure learning |

#### Principe

M9 mod√©lise les d√©pendances conditionnelles entre num√©ros via un graphe acyclique dirig√© (DAG). Contrairement √† M5 qui analyse les co-occurrences par paires de mani√®re ¬´ plate ¬ª, un r√©seau bay√©sien capture les d√©pendances conditionnelles (ex: le num√©ro A est corr√©l√© √† B seulement quand C est pr√©sent).

La structure du r√©seau est apprise √† partir des donn√©es via l'algorithme PC (tests d'ind√©pendance conditionnelle) ou un score BIC/BDeu. L'inf√©rence produit des probabilit√©s mises √† jour √©tant donn√© l'√©vidence partielle.

Pour la pr√©diction du prochain tirage, le mod√®le utilise la propagation de croyances (belief propagation) pour estimer la distribution marginale de chaque num√©ro.

#### Param√®tres

- **structure_algo** (str, d√©faut: 'hc') : Algorithme d'apprentissage de structure : 'hc' (Hill Climbing), 'pc', 'mmhc'.
- **scoring** (str, d√©faut: 'bic') : Score pour la recherche de structure : 'bic', 'bdeu', 'k2'.
- **max_parents** (int, d√©faut: 3) : Nombre maximum de parents par n≈ìud (r√©gularisation).
- **significance_level** (float, d√©faut: 0.05) : Seuil pour les tests d'ind√©pendance (algo PC).

#### Formule / Pseudo-code

```
# Apprentissage de structure :
dag = HillClimbSearch(data).estimate(
        scoring_method=BicScore(data), max_parents=3)

# Estimation des param√®tres :
model = BayesianNetwork(dag.edges())
model.fit(data, estimator=BayesianEstimator, prior_type='BDeu')

# Inf√©rence :
inference = VariableElimination(model)
P(i) = inference.query(['num_i'], evidence={})
```

#### Avantages

- Capture des d√©pendances conditionnelles impossibles √† voir avec de simples co-occurrences.
- Structure apprise interpr√©table visuellement (graphe).
- Cadre bay√©sien avec gestion naturelle de l'incertitude.
- L'absence de liens dans le DAG confirme l'ind√©pendance (support √† H0).

#### Notes d'impl√©mentation

Utiliser pgmpy pour la construction et l'inf√©rence. Encoder les tirages comme une matrice binaire (K colonnes, une par num√©ro). Le DAG appris doit √™tre stock√© et visualis√© (networkx/graphviz) pour l'analyse. Attention : avec K=35-47 num√©ros, l'espace de recherche est large. Limiter max_parents et utiliser des heuristiques.

---

### M12 ‚Äî Mixture of Dirichlet

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M12 |
| **Nom complet** | Mixture of Dirichlet |
| **Type** | M√©lange de distributions Dirichlet |
| **Librairies Python** | numpy, scipy |
| **Complexit√©** | O(N¬∑K¬∑C) par it√©ration EM, C=composantes |

#### Principe

Extension de M1 : au lieu d'un seul prior Dirichlet repr√©sentant un r√©gime unique, M12 utilise un m√©lange de K distributions Dirichlet. Chaque composante du m√©lange repr√©sente un ¬´ mode ¬ª potentiel de la loterie.

L'estimation se fait par l'algorithme EM : E-step assigne chaque tirage √† une composante (soft assignment), M-step met √† jour les param√®tres de chaque Dirichlet.

La pr√©diction est la mixture pond√©r√©e des K composantes. Le nombre de composantes K est s√©lectionn√© par BIC ou WAIC.

Ce mod√®le est plus expressif que M1 et compl√©mentaire √† M4 (HMM) : M4 capture les transitions temporelles entre r√©gimes, M12 capture l'h√©t√©rog√©n√©it√© non temporelle.

#### Param√®tres

- **n_components** (int, d√©faut: 2) : Nombre de composantes du m√©lange. Tester 2 √† 5, s√©lectionner par BIC.
- **alpha_prior** (float, d√©faut: 1.0) : Prior de concentration pour chaque composante.
- **n_iter** (int, d√©faut: 100) : It√©rations EM maximum.
- **tol** (float, d√©faut: 1e-4) : Convergence EM.

#### Formule / Pseudo-code

```
# M√©lange :
P(x) = Œ£_k œÄ_k * Dirichlet(x | Œ±_k)

# E-step : responsabilit√© de chaque composante
r_ik = œÄ_k * Dir(x_i | Œ±_k) / Œ£_j œÄ_j * Dir(x_i | Œ±_j)

# M-step : mise √† jour des param√®tres
œÄ_k = Œ£_i r_ik / N
Œ±_k = MLE ou fixed-point iterations

# Pr√©diction :
P(num) = Œ£_k œÄ_k * E[Dir_k](num)
```

#### Avantages

- Plus expressif qu'un seul Dirichlet (M1) car capture l'h√©t√©rog√©n√©it√©.
- S√©lection de mod√®le rigoureuse par BIC.
- Compl√©mentaire √† M4 (HMM) sans l'aspect temporel.
- Si K=1 optimal, confirme que M1 suffit (support √† H0).

#### Notes d'impl√©mentation

Impl√©menter avec scipy.special (digamma, gammaln) pour les calculs Dirichlet. L'estimation MLE des param√®tres Dirichlet n√©cessite des fixed-point iterations (m√©thode de Minka). Initialiser les composantes par K-means sur les vecteurs de fr√©quences. Attention : la log-vraisemblance Dirichlet peut √™tre instable si Œ± ‚Üí 0.

---

### M13 ‚Äî Spectral / Fourier Analysis

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M13 |
| **Nom complet** | Spectral / Fourier Analysis |
| **Type** | Analyse spectrale des p√©riodicit√©s |
| **Librairies Python** | numpy, scipy.signal, scipy.fft |
| **Complexit√©** | O(N¬∑K¬∑log(N)) |

#### Principe

M13 applique la transform√©e de Fourier rapide (FFT) sur la s√©rie binaire de pr√©sence/absence de chaque num√©ro pour d√©tecter des p√©riodicit√©s √©ventuelles.

Si des cycles existent (li√©s au calendrier des tirages, √† la rotation de machines, ou √† d'autres facteurs syst√©matiques), le spectre de puissance les r√©v√©lera comme des pics significatifs.

La pr√©diction extrapole les composantes fr√©quentielles significatives au temps t+1 par synth√®se harmonique.

M√™me si aucune p√©riodicit√© n'est trouv√©e (ce qui est attendu sous H0), le test spectral a une valeur diagnostique importante pour la th√®se.

#### Param√®tres

- **min_frequency** (float, d√©faut: 0.01) : Fr√©quence minimale √† analyser (inverse de la p√©riode maximale).
- **significance_threshold** (float, d√©faut: 0.01) : Seuil de significativit√© pour la d√©tection de pics spectraux.
- **n_harmonics** (int, d√©faut: 3) : Nombre de composantes fr√©quentielles retenues pour la pr√©diction.
- **detrend** (bool, d√©faut: True) : Retirer la tendance lin√©aire avant la FFT.

#### Formule / Pseudo-code

```
# S√©rie binaire :
x_i(t) = 1{num√©ro i tir√© au tirage t}

# FFT :
X_i(f) = FFT(x_i(t))
PSD_i(f) = |X_i(f)|¬≤ / N

# D√©tection de pics (Fisher's g-test) :
g = max(PSD) / sum(PSD)
p_value = P(g > g_obs | H0)

# Pr√©diction par synth√®se harmonique :
x_pred_i(t+1) = Œ£_h A_h * cos(2œÄ*f_h*(t+1) + œÜ_h)
P(i) ‚àù softmax(x_pred_i(t+1))
```

#### Avantages

- D√©tection de p√©riodicit√©s impossibles √† voir autrement.
- Fondement math√©matique solide (th√©orie spectrale).
- Le Fisher's g-test fournit une p-value exacte pour chaque fr√©quence.
- Valeur diagnostique forte m√™me si aucun signal n'est trouv√© (confirme H0).

#### Notes d'impl√©mentation

Utiliser numpy.fft.rfft pour la FFT et scipy.signal pour le PSD. Appliquer le g-test de Fisher pour la significativit√© des pics (implem custom, ~20 lignes). Si aucun pic significatif, retourner la distribution uniforme (fallback M0). Stocker le spectre complet pour visualisation dans les rapports.

---

### M14 ‚Äî Copula Model

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M14 |
| **Nom complet** | Copula Model |
| **Type** | Mod√©lisation de d√©pendances par copules |
| **Librairies Python** | copulas ou pyvinecopulib, numpy, scipy |
| **Complexit√©** | O(N¬∑K¬≤) ajustement + O(S¬∑K) simulation |

#### Principe

M14 mod√©lise les d√©pendances entre num√©ros en s√©parant les distributions marginales (fr√©quence individuelle de chaque num√©ro) de la structure de d√©pendance (copule). Cela permet une mod√©lisation plus flexible que les corr√©lations lin√©aires utilis√©es par M5.

Le mod√®le estime d'abord les marginales empiriques de chaque num√©ro, les transforme en distributions uniformes [0,1] via la CDF empirique, puis ajuste une copule (gaussienne ou vine) sur les donn√©es transform√©es.

Pour la pr√©diction, on √©chantillonne depuis la copule ajust√©e et on retransforme vers l'espace original pour obtenir des probabilit√©s jointes.

Particuli√®rement utile si les d√©pendances entre num√©ros sont non lin√©aires ou asym√©triques (d√©pendance dans les queues).

#### Param√®tres

- **copula_type** (str, d√©faut: 'gaussian') : Type de copule : 'gaussian', 'student_t', 'vine', 'clayton', 'gumbel'.
- **n_simulations** (int, d√©faut: 10000) : Nombre de simulations pour estimer les probabilit√©s jointes.
- **marginal_method** (str, d√©faut: 'empirical') : M√©thode d'estimation des marginales : 'empirical', 'beta', 'kde'.
- **selection_criterion** (str, d√©faut: 'aic') : Crit√®re de s√©lection de la copule : 'aic', 'bic'.

#### Formule / Pseudo-code

```
# 1. Marginales empiriques :
u_i = F_hat_i(x_i) = rang(x_i) / (N+1)

# 2. Ajustement de la copule :
C(u_1, ..., u_K ; Œ∏) = copule ajust√©e sur [u_1...u_K]

# 3. Simulation :
[v_1...v_K] ~ C(Œ∏)  (n_simulations fois)
x_sim_i = F_hat_i^{-1}(v_i)

# 4. Probabilit√©s :
P(i) = mean(x_sim_i > seuil) ‚àÄ simulations
```

#### Avantages

- Capture des d√©pendances non lin√©aires et asym√©triques.
- S√©paration propre entre structure marginale et structure de d√©pendance.
- Plusieurs types de copules testables, s√©lection par AIC/BIC.
- Si la copule gaussienne est optimale avec corr√©lation nulle, cela confirme l'ind√©pendance (H0).

#### Notes d'impl√©mentation

Utiliser pyvinecopulib ou copulas (pip) pour l'ajustement. Pour les loteries avec K=35-47 num√©ros, une copule gaussienne compl√®te est impraticable (matrice K√óK). Utiliser une copule vine (par paires) ou r√©duire la dimensionnalit√© par PCA. Alternative : travailler sur des groupes de num√©ros (bas/haut, pair/impair) plut√¥t que sur chaque num√©ro individuellement. La dimension √©lev√©e est le d√©fi principal de ce mod√®le.

---

### M15 ‚Äî Thompson Sampling

| Champ | Valeur |
|-------|--------|
| **Identifiant** | M15 |
| **Nom complet** | Thompson Sampling |
| **Type** | Bandit multi-bras bay√©sien |
| **Librairies Python** | numpy |
| **Complexit√©** | O(N¬∑K) pour fit, O(K) pour predict |

#### Principe

M15 traite chaque num√©ro comme un ¬´ bras ¬ª d'un probl√®me de bandit multi-bras. Pour chaque num√©ro, on maintient une distribution Beta(Œ±, Œ≤) mise √† jour √† chaque tirage : Œ± augmente quand le num√©ro est tir√©, Œ≤ augmente quand il ne l'est pas.

La s√©lection se fait par √©chantillonnage : on tire un Œ∏_i de chaque Beta(Œ±_i, Œ≤_i) et on s√©lectionne les K num√©ros avec les plus grands Œ∏.

L'avantage de Thompson Sampling est son √©quilibre naturel exploration/exploitation : un num√©ro peu observ√© a une distribution large (forte incertitude), donc il peut √™tre s√©lectionn√© par surprise. Cela donne de la diversit√© aux combinaisons g√©n√©r√©es.

Pour la production de probabilit√©s (predict_proba), on utilise la moyenne de la distribution Beta : E[Œ∏_i] = Œ±_i / (Œ±_i + Œ≤_i).

#### Param√®tres

- **prior_alpha** (float, d√©faut: 1.0) : Prior Œ± initial (1.0 = prior uniforme / non informatif).
- **prior_beta** (float, d√©faut: 1.0) : Prior Œ≤ initial.
- **decay_factor** (float, d√©faut: 1.0) : Facteur de d√©croissance appliqu√© √† Œ± et Œ≤ √† chaque pas (1.0 = pas de d√©croissance, 0.99 = oubli progressif).
- **n_samples** (int, d√©faut: 1000) : Nombre d'√©chantillonnages pour estimer les probabilit√©s moyennes.

#### Formule / Pseudo-code

```
# Initialisation :
Œ±_i = prior_alpha, Œ≤_i = prior_beta  ‚àÄi

# Mise √† jour apr√®s chaque tirage :
Œ±_i *= decay_factor ; Œ≤_i *= decay_factor
Œ±_i += 1 si i tir√©, sinon Œ≤_i += 1

# G√©n√©ration de combinaisons :
Œ∏_i ~ Beta(Œ±_i, Œ≤_i)  ‚àÄi
Combos = top-K(Œ∏)

# Probabilit√©s :
P(i) = E[Œ∏_i] = Œ±_i / (Œ±_i + Œ≤_i), normalis√©
```

#### Avantages

- √âquilibre exploration/exploitation naturel et th√©oriquement fond√©.
- Chaque appel √† generate_combinations produit une combinaison diff√©rente (stochastique).
- Le decay_factor permet d'oublier progressivement l'historique ancien.
- Cadre d√©cisionnel bien √©tudi√© avec regret bounds th√©oriques.
- Impl√©mentation tr√®s simple et rapide.

#### Notes d'impl√©mentation

Impl√©mentation directe avec numpy (np.random.beta). Le decay_factor est critique : sans decay, les distributions deviennent tr√®s concentr√©es apr√®s de nombreux tirages et l'exploration dispara√Æt. Pour le backtest, stocker la s√©rie compl√®te des (Œ±_i, Œ≤_i) pour analyse.

---

## 5. Tableau r√©capitulatif complet

Ce tableau synth√©tise tous les mod√®les (existants + nouveaux) avec leur positionnement.

| ID | Nom | Approche | Hypoth√®se test√©e | Librairie | Statut |
|----|-----|----------|-------------------|-----------|--------|
| **M0** | Baseline | Uniforme | R√©f√©rence (H0) | numpy | ‚úÖ Existant |
| **M1** | Dirichlet | Bay√©sien | Biais de fr√©quence | numpy | ‚úÖ Existant |
| **M2** | Windowed | Fen√™tre fixe | Tendance r√©cente | numpy | ‚úÖ Existant |
| **M3** | Exp. Decay | Pond√©ration exp. | D√©rive lente | numpy | üî∂ √Ä impl. |
| **M4** | HMM | √âtats cach√©s | R√©gimes multiples | hmmlearn | üî∂ √Ä impl. |
| **M5** | Co-occurrence | Paires | Corr√©lations | numpy | ‚úÖ Existant |
| **M6** | Gaps & Streaks | √âcarts | Retard / rattrapage | numpy | ‚úÖ Existant |
| **M7** | Entropy | Entropie locale | Pr√©visibilit√© locale | scipy | üî∂ √Ä impl. |
| **M8** | Changepoint | PELT/CUSUM | Stationnarit√© | ruptures | üî∂ √Ä impl. |
| **M9** | Bayes Net | DAG + inf√©rence | D√©pendances cond. | pgmpy | üî∂ √Ä impl. |
| **M10** | Ensemble | Stacking | Consensus | numpy | ‚úÖ Existant |
| **M15** | Thompson | Bandit | Explore/exploit | numpy | üî∂ √Ä impl. |
| **M12** | Mix Dirichlet | M√©lange EM | H√©t√©rog√©n√©it√© | scipy | üî∂ √Ä impl. |
| **M13** | Spectral | FFT | P√©riodicit√©s | numpy/scipy | üî∂ √Ä impl. |
| **M14** | Copula | Copules | D√©p. non lin√©aires | copulas | üî∂ √Ä impl. |
| **ANTI** | Anti-Consensus | Contrarian | Sous-estimation | numpy | ‚úÖ Existant |
| **ANTI2** | Anti-Cons. v2 | Contrarian+ | Diversit√© | numpy | ‚úÖ Existant |

---

*‚Äî Fin du cahier des charges ‚Äî*

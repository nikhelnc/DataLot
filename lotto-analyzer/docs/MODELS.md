# ğŸ“Š ModÃ¨les de PrÃ©diction - Lotto Analyzer

Ce document dÃ©crit les diffÃ©rents modÃ¨les statistiques utilisÃ©s pour l'analyse et la simulation des tirages de loterie.

---

## ğŸ¯ Vue d'ensemble

Le systÃ¨me utilise plusieurs modÃ¨les de prÃ©diction, chacun basÃ© sur une approche statistique diffÃ©rente. Ces modÃ¨les sont utilisÃ©s dans le module de **backtest** pour Ã©valuer leur performance historique et gÃ©nÃ©rer des prÃ©dictions pour les prochains tirages.

| ModÃ¨le | Nom | Type | Description courte |
|--------|-----|------|-------------------|
| M0 | Baseline | AlÃ©atoire | SÃ©lection alÃ©atoire uniforme |
| M1 | Dirichlet | BayÃ©sien | Estimation bayÃ©sienne des probabilitÃ©s |
| M2 | Windowed | FenÃªtre glissante | PondÃ©ration rÃ©cente avec shrinkage |
| M3 | Exponential Decay | PondÃ©ration temporelle | DÃ©croissance exponentielle des frÃ©quences |
| M4 | HMM | ModÃ¨le Ã  Ã©tats cachÃ©s | Hidden Markov Model pour rÃ©gimes |
| M5 | Co-occurrence | Analyse de paires | Paires de numÃ©ros sur-reprÃ©sentÃ©es |
| M6 | Gaps & Streaks | Analyse des Ã©carts | NumÃ©ros "en retard" |
| M7 | Entropy | SÃ©lection entropique | BasÃ© sur l'entropie locale |
| M8 | Changepoint | DÃ©tection de ruptures | Analyse post-rupture structurelle |
| M9 | Bayesian Network | RÃ©seau bayÃ©sien | DÃ©pendances conditionnelles |
| M10 | Ensemble | MÃ©ta-modÃ¨le | Combinaison de M0, M1, M2 |
| M11 | LSTM Hybrid | Deep Learning | LSTM + Attention + Embeddings |
| M12 | Mixture Dirichlet | MÃ©lange bayÃ©sien | MÃ©lange de distributions Dirichlet |
| M13 | Spectral | Analyse de Fourier | DÃ©tection de pÃ©riodicitÃ©s |
| M14 | Copula | ModÃ¨le de dÃ©pendance | Copules pour corrÃ©lations |
| M15 | Thompson Sampling | Bandit multi-bras | Exploration/exploitation bayÃ©sienne |
| M16 | Gradient Boosting | Machine Learning | XGBoost avec features engineered |
| M17 | Autoencoder Anomaly | Deep Learning | DÃ©tection d'anomalies par autoencoder |
| M18 | Graph Neural Network | Deep Learning | GNN sur graphe de co-occurrences |
| M19 | Temporal Fusion | Multi-Ã©chelle | Fusion de features multi-temporelles |
| M20 | Meta-Learner | MÃ©ta-apprentissage | SÃ©lection dynamique des meilleurs modÃ¨les |
| ANTI | Anti-Consensus | Contrarian | NumÃ©ros NON prÃ©dits par les autres modÃ¨les |
| ANTI2 | Anti-Consensus v2 | Contrarian + DiversitÃ© | ANTI avec contrainte de diversitÃ© |

---

## ğŸ“ˆ ModÃ¨les DÃ©taillÃ©s

### M0 - Baseline (AlÃ©atoire)

**Type** : SÃ©lection alÃ©atoire uniforme

**Principe** :
- SÃ©lectionne les numÃ©ros de maniÃ¨re complÃ¨tement alÃ©atoire
- Chaque numÃ©ro a une probabilitÃ© Ã©gale d'Ãªtre choisi
- Sert de **rÃ©fÃ©rence** pour comparer les autres modÃ¨les

**UtilitÃ©** :
- Ã‰tablir un taux de rÃ©ussite de base (baseline)
- Tout modÃ¨le performant doit faire mieux que M0 sur le long terme

**Formule** :
```
P(numÃ©ro i) = 1 / N  (oÃ¹ N = nombre total de numÃ©ros)
```

---

### M1 - Dirichlet (BayÃ©sien)

**Type** : Estimation bayÃ©sienne avec prior Dirichlet

**Principe** :
- Utilise un prior Dirichlet pour estimer les probabilitÃ©s de chaque numÃ©ro
- Le prior permet de "lisser" les estimations quand on a peu de donnÃ©es
- Les numÃ©ros frÃ©quemment tirÃ©s ont une probabilitÃ© plus Ã©levÃ©e

**ParamÃ¨tres** :
- `alpha` : ParamÃ¨tre de concentration du prior (dÃ©faut: 1 = prior uniforme)

**Avantages** :
- Robuste avec peu de donnÃ©es historiques
- Ã‰vite les probabilitÃ©s nulles pour les numÃ©ros jamais tirÃ©s

**Formule** :
```
P(numÃ©ro i) = (count_i + alpha) / (total_tirages + N * alpha)
```

---

### M2 - Windowed (FenÃªtre Glissante)

**Type** : Estimation avec fenÃªtre temporelle et shrinkage

**Principe** :
- Donne plus de poids aux tirages rÃ©cents
- Utilise une fenÃªtre glissante pour capturer les tendances rÃ©centes
- Applique un "shrinkage" (Î») vers la moyenne globale pour Ã©viter le surapprentissage

**ParamÃ¨tres** :
- `window_size` : Taille de la fenÃªtre (dÃ©faut: 50 tirages)
- `lambda_shrink` : Facteur de shrinkage (dÃ©faut: 0.1)

**Avantages** :
- Capture les tendances rÃ©centes
- Ã‰quilibre entre donnÃ©es rÃ©centes et historique complet

**Formule** :
```
P(numÃ©ro i) = Î» * P_global(i) + (1-Î») * P_fenÃªtre(i)
```

---

### M5 - Co-occurrence (Analyse de Paires)

**Type** : Analyse des paires de numÃ©ros

**Principe** :
- Identifie les paires de numÃ©ros qui apparaissent ensemble plus souvent que prÃ©vu
- Calcule le "delta" entre frÃ©quence observÃ©e et frÃ©quence attendue
- SÃ©lectionne les numÃ©ros prÃ©sents dans les paires les plus sur-reprÃ©sentÃ©es

**MÃ©thode** :
1. Calculer la frÃ©quence de chaque paire (i, j)
2. Calculer la frÃ©quence attendue : `P(i) * P(j) * n_tirages`
3. Delta = ObservÃ© - Attendu
4. SÃ©lectionner les numÃ©ros des top 20 paires avec le plus grand delta

**Avantages** :
- Capture les corrÃ©lations entre numÃ©ros
- Peut dÃ©tecter des patterns non Ã©vidents

---

### M6 - Gaps & Streaks (Analyse des Ã‰carts)

**Type** : Analyse des Ã©carts et sÃ©ries

**Principe** :
- Identifie les numÃ©ros "en retard" (overdue) qui n'ont pas Ã©tÃ© tirÃ©s depuis longtemps
- Compare l'Ã©cart actuel Ã  l'Ã©cart moyen historique
- SÃ©lectionne les numÃ©ros avec le plus grand Ã©cart positif (delta_gap)

**MÃ©thode** :
1. Pour chaque numÃ©ro, calculer l'Ã©cart actuel (tirages depuis derniÃ¨re apparition)
2. Calculer l'Ã©cart moyen historique
3. Delta = Ã‰cart actuel - Ã‰cart moyen
4. SÃ©lectionner les numÃ©ros avec le plus grand delta positif

**HypothÃ¨se** :
- Les numÃ©ros "en retard" ont une probabilitÃ© accrue d'apparaÃ®tre (loi des grands nombres)

**Note** : Cette hypothÃ¨se est controversÃ©e (gambler's fallacy), mais peut capturer des patterns rÃ©els dans certains systÃ¨mes.

---

### M10 - Ensemble (Stacking)

**Type** : MÃ©ta-modÃ¨le combinant plusieurs modÃ¨les

**Principe** :
- Combine les prÃ©dictions de M0, M1 et M2
- Utilise une technique de stacking pour pondÃ©rer les modÃ¨les
- Produit une prÃ©diction consensus

**MÃ©thode** :
1. Obtenir les probabilitÃ©s de M0, M1, M2
2. Combiner avec des poids optimisÃ©s
3. SÃ©lectionner les numÃ©ros avec la probabilitÃ© combinÃ©e la plus Ã©levÃ©e

**Avantages** :
- RÃ©duit la variance des prÃ©dictions individuelles
- Plus robuste que les modÃ¨les individuels

---

### M11 - LSTM Hybrid (Deep Learning)

**Type** : RÃ©seau de neurones rÃ©current avec attention

**Principe** :
- Utilise un **LSTM Bidirectionnel** pour capturer la sÃ©quentialitÃ© temporelle des tirages
- IntÃ¨gre un **mÃ©canisme d'Attention Multi-Head** pour identifier les tirages passÃ©s les plus pertinents
- Emploie des **Embeddings** pour apprendre les relations latentes entre numÃ©ros
- GÃ©nÃ¨re une **carte de chaleur de probabilitÃ©** pour tous les numÃ©ros possibles

**Architecture** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BLOC 1: Ingestion                                      â”‚
â”‚  â”œâ”€â”€ Input SÃ©quence: (50 derniers tirages Ã— N numÃ©ros) â”‚
â”‚  â”œâ”€â”€ Embeddings: Projection en espace vectoriel (dim=32)â”‚
â”‚  â””â”€â”€ MÃ©ta-Features: Somme, Ã©cart-type, pairs, etc.     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BLOC 2: Cerveau Temporel                               â”‚
â”‚  â”œâ”€â”€ LSTM Bidirectionnel (64 unitÃ©s)                   â”‚
â”‚  â”œâ”€â”€ Multi-Head Attention (2 tÃªtes)                    â”‚
â”‚  â””â”€â”€ Layer Normalization + RÃ©siduel                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BLOC 3: TÃªte de PrÃ©diction                            â”‚
â”‚  â”œâ”€â”€ Dense (128) + Dropout (0.3)                       â”‚
â”‚  â”œâ”€â”€ Dense (64) + Dropout (0.3)                        â”‚
â”‚  â””â”€â”€ Sortie Sigmoid (N probabilitÃ©s indÃ©pendantes)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ParamÃ¨tres** :
- `sequence_length` : Nombre de tirages passÃ©s utilisÃ©s (dÃ©faut: 50)
- `embedding_dim` : Dimension des embeddings (dÃ©faut: 32)
- `lstm_units` : UnitÃ©s LSTM (dÃ©faut: 64)
- `attention_heads` : TÃªtes d'attention (dÃ©faut: 2)
- `dropout_rate` : Taux de dropout (dÃ©faut: 0.3)
- `epochs` : Ã‰poques d'entraÃ®nement (dÃ©faut: 30)

**MÃ©ta-Features calculÃ©es** :
1. Somme des numÃ©ros (normalisÃ©e)
2. Ã‰cart-type (normalisÃ©)
3. Ratio de numÃ©ros pairs
4. Ratio de numÃ©ros bas (< mÃ©diane)
5. Ã‰tendue (max - min, normalisÃ©e)

**Fonction de coÃ»t** :
- `binary_crossentropy` : Traite le problÃ¨me comme N classifications binaires indÃ©pendantes

**Avantages** :
- Capture les patterns temporels complexes
- L'attention permet de "voir" des motifs Ã  longue distance
- Les embeddings peuvent rÃ©vÃ©ler des biais mÃ©caniques (si le jeu n'est pas parfaitement alÃ©atoire)
- Fallback automatique vers estimation frÃ©quentielle si TensorFlow non disponible

**HypothÃ¨se testable** :
Si le gÃ©nÃ©rateur de nombres est parfait, les embeddings resteront orthogonaux. S'ils se regroupent (clustering), cela pourrait indiquer un biais.

**Note** : Ce modÃ¨le est plus lent Ã  entraÃ®ner que les autres. Il nÃ©cessite TensorFlow.

---

### M3 - Exponential Decay

**Type** : PondÃ©ration temporelle avec dÃ©croissance exponentielle

**Principe** :
- PondÃ¨re les tirages rÃ©cents plus fortement que les anciens
- Utilise une fonction de dÃ©croissance exponentielle : `w(t) = exp(-Î» * (T - t))`
- Les numÃ©ros frÃ©quents dans les tirages rÃ©cents ont une probabilitÃ© plus Ã©levÃ©e

**ParamÃ¨tres** :
- `lambda_decay` : Taux de dÃ©croissance (dÃ©faut: 0.02)
- `normalize` : Normaliser les poids (dÃ©faut: true)

**Formule** :
```
P(numÃ©ro i) = Î£ w(t) * I(i âˆˆ tirage_t) / Î£ w(t)
oÃ¹ w(t) = exp(-Î» * (T - t))
```

**Avantages** :
- Adaptatif aux changements rÃ©cents
- Simple et interprÃ©table
- Pas d'hypothÃ¨se sur la stationnaritÃ©

---

### M4 - HMM (Hidden Markov Model)

**Type** : ModÃ¨le Ã  Ã©tats cachÃ©s

**Principe** :
- ModÃ©lise les tirages comme provenant de diffÃ©rents "rÃ©gimes" latents
- Chaque Ã©tat a sa propre distribution de probabilitÃ© sur les numÃ©ros
- PrÃ©dit l'Ã©tat suivant et utilise sa distribution d'Ã©mission

**ParamÃ¨tres** :
- `n_states` : Nombre d'Ã©tats cachÃ©s (dÃ©faut: 3)
- `n_iter` : ItÃ©rations max pour Baum-Welch (dÃ©faut: 100)

**Formule** :
```
P(numÃ©ro i) = Î£ P(Ã©tat_k | observations) * P(numÃ©ro i | Ã©tat_k)
```

**Avantages** :
- Capture les changements de rÃ©gime
- ModÃ¨le probabiliste complet
- Fallback disponible si hmmlearn non installÃ©

**DÃ©pendance** : `hmmlearn` (optionnel)

---

### M7 - Entropy-Based Selection

**Type** : SÃ©lection basÃ©e sur l'entropie locale

**Principe** :
- Calcule l'entropie de Shannon pour chaque numÃ©ro sur une fenÃªtre glissante
- Favorise les numÃ©ros avec une entropie plus faible (comportement plus prÃ©visible)
- Compare Ã  l'entropie thÃ©orique sous H0

**ParamÃ¨tres** :
- `window_size` : Taille de la fenÃªtre (dÃ©faut: 30)
- `selection_mode` : 'low_entropy' ou 'high_entropy'
- `temperature` : TempÃ©rature softmax (dÃ©faut: 1.0)

**Formule** :
```
H_i = -p_i * log2(p_i) - (1-p_i) * log2(1-p_i)
score_i = H0 - H_i
P(i) âˆ softmax(score_i / temperature)
```

**Avantages** :
- DÃ©tecte les numÃ©ros avec comportement anormal
- BasÃ© sur la thÃ©orie de l'information

---

### M8 - Changepoint Detection

**Type** : DÃ©tection de ruptures structurelles

**Principe** :
- DÃ©tecte les points de rupture dans les sÃ©ries de frÃ©quences
- Utilise l'algorithme PELT (Pruned Exact Linear Time)
- Recalcule les probabilitÃ©s uniquement sur le segment post-rupture

**ParamÃ¨tres** :
- `model_type` : ModÃ¨le de coÃ»t ('l1', 'l2', 'rbf')
- `min_segment_size` : Taille minimale de segment (dÃ©faut: 20)
- `penalty` : PÃ©nalitÃ© PELT ('bic', 'aic')

**Avantages** :
- Adaptatif aux changements structurels
- Ignore les donnÃ©es obsolÃ¨tes
- DÃ©tection automatique des ruptures

**DÃ©pendance** : `ruptures`

---

### M9 - Bayesian Network

**Type** : RÃ©seau bayÃ©sien pour dÃ©pendances conditionnelles

**Principe** :
- ModÃ©lise les dÃ©pendances entre numÃ©ros via un DAG
- Apprend la structure du rÃ©seau par Hill Climbing
- Calcule les probabilitÃ©s marginales par infÃ©rence

**ParamÃ¨tres** :
- `structure_algo` : Algorithme de structure ('hc', 'pc')
- `max_parents` : Parents max par nÅ“ud (dÃ©faut: 3)
- `n_top_numbers` : Nombre de numÃ©ros Ã  modÃ©liser (dÃ©faut: 15)

**Avantages** :
- Capture les corrÃ©lations entre numÃ©ros
- InterprÃ©table (visualisation du DAG)
- Fallback par corrÃ©lations si pgmpy non disponible

**DÃ©pendance** : `pgmpy` (optionnel)

---

### M12 - Mixture of Dirichlet

**Type** : MÃ©lange de distributions Dirichlet

**Principe** :
- Utilise plusieurs composantes Dirichlet pour capturer l'hÃ©tÃ©rogÃ©nÃ©itÃ©
- Chaque composante reprÃ©sente un "mode" potentiel
- Estimation par algorithme EM

**ParamÃ¨tres** :
- `n_components` : Nombre de composantes (dÃ©faut: 2)
- `alpha_prior` : Prior de concentration (dÃ©faut: 1.0)
- `n_iter` : ItÃ©rations EM max (dÃ©faut: 100)

**Formule** :
```
P(numÃ©ro i) = Î£ Ï€_k * E[Î¸_i | Î±_k]
oÃ¹ Ï€_k sont les poids du mÃ©lange
```

**Avantages** :
- Plus flexible qu'un simple Dirichlet
- Capture les modes multiples

---

### M13 - Spectral / Fourier Analysis

**Type** : Analyse spectrale pour dÃ©tection de pÃ©riodicitÃ©s

**Principe** :
- Applique la FFT sur les sÃ©ries binaires de prÃ©sence/absence
- DÃ©tecte les frÃ©quences significatives (test de Fisher)
- Extrapole les harmoniques pour prÃ©dire

**ParamÃ¨tres** :
- `min_frequency` : FrÃ©quence minimale Ã  analyser
- `n_harmonics` : Nombre d'harmoniques Ã  retenir (dÃ©faut: 3)
- `detrend` : Retirer la tendance linÃ©aire (dÃ©faut: true)

**Formule** :
```
X_i(f) = FFT(x_i(t))
PSD_i(f) = |X_i(f)|Â² / N
```

**Avantages** :
- DÃ©tecte les cycles cachÃ©s
- BasÃ© sur l'analyse de Fourier classique

---

### M14 - Copula Model

**Type** : ModÃ©lisation des dÃ©pendances par copules

**Principe** :
- SÃ©pare les distributions marginales de la structure de dÃ©pendance
- Utilise une copule gaussienne pour modÃ©liser les corrÃ©lations
- Simule pour estimer les probabilitÃ©s

**ParamÃ¨tres** :
- `copula_type` : Type de copule ('gaussian')
- `n_simulations` : Nombre de simulations (dÃ©faut: 10000)
- `n_groups` : Groupes de numÃ©ros (dÃ©faut: 5)

**Avantages** :
- ModÃ©lise les dÃ©pendances non-linÃ©aires
- Flexible sur les marginales
- Fallback par corrÃ©lations si copulas non disponible

**DÃ©pendance** : `copulas` (optionnel)

---

### M15 - Thompson Sampling

**Type** : Bandit multi-bras bayÃ©sien

**Principe** :
- Traite chaque numÃ©ro comme un bras de bandit
- Maintient une distribution Beta(Î±, Î²) pour chaque numÃ©ro
- Ã‰chantillonne pour Ã©quilibrer exploration/exploitation

**ParamÃ¨tres** :
- `alpha_prior` : Prior Î± (dÃ©faut: 1.0)
- `beta_prior` : Prior Î² (dÃ©faut: 1.0)
- `n_samples` : Ã‰chantillons Thompson (dÃ©faut: 1000)

**Formule** :
```
Î¸_i ~ Beta(Î±_i + succÃ¨s_i, Î²_i + Ã©checs_i)
P(numÃ©ro i) âˆ E[Î¸_i]
```

**Avantages** :
- Ã‰quilibre exploration et exploitation
- Approche bayÃ©sienne naturelle
- Converge vers les vraies probabilitÃ©s

---

### ANTI - Anti-Consensus

**Type** : StratÃ©gie contrariante

**Principe** :
- Identifie les numÃ©ros **NON prÃ©dits** par les autres modÃ¨les (M0, M1, M2, M5, M6, M10)
- GÃ©nÃ¨re des combinaisons Ã  partir de ces numÃ©ros "ignorÃ©s"
- HypothÃ¨se : si tous les modÃ¨les se trompent, les numÃ©ros ignorÃ©s ont plus de chances

**ParamÃ¨tres** :
- `n_combinations` : Nombre de combinaisons Ã  gÃ©nÃ©rer par tirage (dÃ©faut: 10)

**MÃ©thode** :
1. Collecter tous les numÃ©ros prÃ©dits par les autres modÃ¨les
2. Identifier les numÃ©ros restants (non prÃ©dits)
3. GÃ©nÃ©rer N combinaisons alÃ©atoires Ã  partir de ces numÃ©ros

**Avantages** :
- Diversification par rapport aux autres modÃ¨les
- Peut capturer des numÃ©ros systÃ©matiquement sous-estimÃ©s

---

### ANTI2 - Anti-Consensus v2 (avec DiversitÃ©)

**Type** : StratÃ©gie contrariante avec contrainte de diversitÃ©

**Principe** :
- MÃªme logique que ANTI, mais avec une contrainte supplÃ©mentaire
- Les combinaisons gÃ©nÃ©rÃ©es doivent Ãªtre **diversifiÃ©es** entre elles
- Ã‰vite d'avoir trop de numÃ©ros en commun entre les combinaisons

**ParamÃ¨tres** :
- `n_combinations` : Nombre de combinaisons Ã  gÃ©nÃ©rer par tirage (dÃ©faut: 10)
- `max_common_main` : Nombre maximum de numÃ©ros principaux identiques entre deux combinaisons (dÃ©faut: 2)
- `max_common_bonus` : Nombre maximum de numÃ©ros bonus identiques entre deux combinaisons (dÃ©faut: 0)

**MÃ©thode** :
1. Collecter tous les numÃ©ros prÃ©dits par les autres modÃ¨les
2. Identifier les numÃ©ros restants (non prÃ©dits)
3. Pour chaque combinaison Ã  gÃ©nÃ©rer :
   - GÃ©nÃ©rer une combinaison candidate
   - VÃ©rifier qu'elle ne partage pas trop de numÃ©ros avec les combinaisons dÃ©jÃ  gÃ©nÃ©rÃ©es
   - Si OK, l'ajouter ; sinon, rÃ©essayer (max 100 tentatives)
4. Si `max_common_bonus = 0`, chaque combinaison aura un numÃ©ro bonus unique

**Avantages** :
- Maximise la couverture des numÃ©ros possibles
- Avec `max_common_bonus = 0`, garantit des bonus tous diffÃ©rents
- Meilleure diversification du portefeuille de combinaisons

**Exemple** :
Avec `max_common_main = 2` et `max_common_bonus = 0` :
```
Combo #1: [5, 12, 23, 34, 45] + [7]
Combo #2: [8, 12, 19, 34, 41] + [3]   â† max 2 numÃ©ros en commun (12, 34), bonus diffÃ©rent
Combo #3: [3, 15, 27, 38, 49] + [11]  â† bonus unique
```

---

### M16 - Gradient Boosting Ensemble

**Type** : Machine Learning (XGBoost/LightGBM)

**Principe** :
- Utilise le gradient boosting pour prÃ©dire les probabilitÃ©s de chaque numÃ©ro
- IngÃ©nierie de features avancÃ©e : frÃ©quences, gaps, co-occurrences, statistiques temporelles
- EntraÃ®nement walk-forward pour Ã©viter le surapprentissage

**Features calculÃ©es** :
1. FrÃ©quence globale, fenÃªtrÃ©e, et avec dÃ©croissance exponentielle
2. Gap actuel, gap moyen, ratio de gap
3. Statistiques des tirages rÃ©cents (somme, Ã©cart-type, Ã©tendue)
4. Position du numÃ©ro (bas/haut), paritÃ©

**ParamÃ¨tres** :
- `n_estimators` : Nombre d'arbres (dÃ©faut: 100)
- `max_depth` : Profondeur maximale (dÃ©faut: 6)
- `learning_rate` : Taux d'apprentissage (dÃ©faut: 0.1)
- `window_size` : FenÃªtre pour les features (dÃ©faut: 50)

**Avantages** :
- Capture des interactions non-linÃ©aires entre features
- Importance des features interprÃ©table
- Robuste au bruit

**DÃ©pendance** : `xgboost` ou `lightgbm` (optionnel)

---

### M17 - Autoencoder Anomaly

**Type** : Deep Learning (DÃ©tection d'anomalies)

**Principe** :
- EntraÃ®ne un autoencoder Ã  reconstruire les patterns "normaux" de tirages
- Les numÃ©ros avec une erreur de reconstruction Ã©levÃ©e sont considÃ©rÃ©s anomaux
- HypothÃ¨se : les anomalies peuvent indiquer des numÃ©ros plus susceptibles d'apparaÃ®tre

**Architecture** :
```
Encoder: Input â†’ Dense(32) â†’ Dense(16) â†’ Latent(16)
Decoder: Latent(16) â†’ Dense(16) â†’ Dense(32) â†’ Output
```

**ParamÃ¨tres** :
- `encoding_dim` : Dimension de l'espace latent (dÃ©faut: 16)
- `hidden_layers` : Couches cachÃ©es (dÃ©faut: [32, 16])
- `epochs` : Ã‰poques d'entraÃ®nement (dÃ©faut: 50)
- `sequence_length` : Longueur des sÃ©quences (dÃ©faut: 20)

**Avantages** :
- DÃ©tecte des patterns subtils non visibles par les mÃ©thodes classiques
- L'espace latent peut rÃ©vÃ©ler des structures cachÃ©es
- Applicable Ã  la dÃ©tection d'anomalies dans les tirages

**DÃ©pendance** : `tensorflow` (optionnel)

---

### M18 - Graph Neural Network

**Type** : Deep Learning (RÃ©seaux de graphes)

**Principe** :
- ModÃ©lise les numÃ©ros comme des nÅ“uds dans un graphe
- Les arÃªtes reprÃ©sentent les co-occurrences entre numÃ©ros
- Utilise le message passing pour apprendre des embeddings de numÃ©ros

**Architecture** :
```
Nodes: NumÃ©ros de loterie (1 Ã  N)
Edges: Co-occurrences pondÃ©rÃ©es
GNN: 2 couches de message passing + sortie sigmoÃ¯de
```

**ParamÃ¨tres** :
- `embedding_dim` : Dimension des embeddings (dÃ©faut: 32)
- `hidden_dim` : Dimension cachÃ©e (dÃ©faut: 64)
- `n_layers` : Nombre de couches GNN (dÃ©faut: 2)
- `epochs` : Ã‰poques d'entraÃ®nement (dÃ©faut: 100)

**Avantages** :
- Capture les relations structurelles entre numÃ©ros
- Les embeddings peuvent Ãªtre analysÃ©s (clustering, visualisation)
- Approche innovante pour l'analyse de loteries

**DÃ©pendance** : `torch` (optionnel)

---

### M19 - Temporal Fusion

**Type** : Multi-Ã©chelle temporelle

**Principe** :
- Combine des informations de plusieurs Ã©chelles temporelles
- Court terme (10 tirages), moyen terme (30 tirages), long terme (100+ tirages)
- Utilise un mÃ©canisme d'attention pour pondÃ©rer les Ã©chelles

**Ã‰chelles** :
1. **Court terme** : Capture les tendances trÃ¨s rÃ©centes
2. **Moyen terme** : Capture les patterns mensuels
3. **Long terme** : Baseline historique stable

**ParamÃ¨tres** :
- `short_window` : FenÃªtre court terme (dÃ©faut: 10)
- `medium_window` : FenÃªtre moyen terme (dÃ©faut: 30)
- `long_window` : FenÃªtre long terme (dÃ©faut: 100)
- `temperature` : TempÃ©rature softmax pour l'attention (dÃ©faut: 1.0)

**Avantages** :
- Adaptatif : donne plus de poids aux Ã©chelles performantes
- Pas de dÃ©pendances externes
- InterprÃ©table : on peut voir les poids de chaque Ã©chelle

---

### M20 - Meta-Learner Adaptive

**Type** : MÃ©ta-apprentissage

**Principe** :
- Maintient un pool de modÃ¨les de base (M1-M19)
- Ã‰value la performance rÃ©cente de chaque modÃ¨le
- Combine dynamiquement les prÃ©dictions avec des poids adaptatifs

**Fonctionnement** :
1. Ã‰value chaque modÃ¨le sur une fenÃªtre de validation
2. Calcule des poids via softmax des scores de performance
3. Combine les prÃ©dictions des top-N modÃ¨les
4. S'adapte au fil du temps

**ParamÃ¨tres** :
- `validation_window` : FenÃªtre de validation (dÃ©faut: 20)
- `n_top_models` : Nombre de modÃ¨les Ã  combiner (dÃ©faut: 5)
- `temperature` : TempÃ©rature pour le softmax (dÃ©faut: 1.0)
- `decay_factor` : Facteur d'oubli (dÃ©faut: 0.95)

**Avantages** :
- SÃ©lection automatique des meilleurs modÃ¨les
- Robuste : combine plusieurs approches
- Adaptatif : s'ajuste aux changements de rÃ©gime
- Fournit un diagnostic sur la performance relative des modÃ¨les

---

## ğŸ“Š MÃ©triques d'Ã‰valuation

### Taux de RÃ©ussite (Hit Rate)

```
Hit Rate = NumÃ©ros corrects / NumÃ©ros Ã  deviner
```

### Lift vs Random

```
Lift = Hit Rate du modÃ¨le / Hit Rate attendu (alÃ©atoire)
```

Un lift > 1 indique que le modÃ¨le fait mieux que le hasard.

### Divisions de Prix

Le systÃ¨me calcule Ã©galement les divisions de prix atteintes par chaque combinaison, basÃ©es sur les rÃ¨gles du jeu (nombre de numÃ©ros principaux + bonus corrects).

---

## ğŸ”§ Utilisation dans le Backtest

1. **SÃ©lectionner les modÃ¨les** Ã  tester dans l'interface
2. **Configurer les paramÃ¨tres** :
   - Nombre de tirages Ã  tester
   - Nombre de combinaisons (pour ANTI/ANTI2)
   - Contraintes de diversitÃ© (pour ANTI2)
3. **Lancer le backtest**
4. **Analyser les rÃ©sultats** :
   - Taux de rÃ©ussite par modÃ¨le
   - Ã‰volution temporelle
   - Divisions de prix atteintes

---

## ğŸ“ Notes Importantes

1. **Aucun modÃ¨le ne peut prÃ©dire les tirages** - Les loteries sont des systÃ¨mes alÃ©atoires
2. **Les performances passÃ©es ne garantissent pas les performances futures**
3. **Ces modÃ¨les sont Ã  but Ã©ducatif et d'analyse statistique**
4. **Le lift > 1 peut Ãªtre dÃ» au hasard** sur un petit Ã©chantillon

---

*Document gÃ©nÃ©rÃ© pour Lotto Analyzer v2.0*
